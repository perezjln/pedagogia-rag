import os
import torch
import logging
import streamlit as st

from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import login
from dotenv import load_dotenv
from smolagents import HfApiModel


# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)


def do_generate(model, tokenizer, retrieved_docs, user_query):
    """
    G√©n√®re une r√©ponse √† partir des documents r√©cup√©r√©s.
    Le prompt est formul√© en fran√ßais et la r√©ponse est g√©n√©r√©e en markdown.
    """
    # Construction du contexte √† partir des documents r√©cup√©r√©s
    context = "\n".join([doc.page_content for doc in retrieved_docs])
    prompt = f"""
Vous √™tes un assistant IA.  
Vous r√©pondez √† la question.  
Votre r√©ponse doit √™tre informative et concise.  
Vous utilisez le contexte suivant pour g√©n√©rer la r√©ponse.  
Vous r√©pondez UNIQUEMENT en format markdown.
Tu r√©ponds UNIQUEMENT en FRAN√áAIS.

Context:
{context}

Question:
{user_query}
    """
    logging.info("G√©n√©ration du prompt pour la g√©n√©ration de r√©ponse.")

    # Si on utilise un mod√®le bas√© sur transformers
    if tokenizer:
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        with torch.no_grad():
            output = model.generate(**inputs, max_new_tokens=200)
        result = tokenizer.decode(output[0], skip_special_tokens=True)
        logging.info("R√©ponse g√©n√©r√©e avec le mod√®le transformers.")
        return result

    # Sinon, utilisation de HfApiModel
    messages = [{"role": "user", "content": prompt}]
    response = model(messages, stop_sequences=["END"])
    logging.info("R√©ponse g√©n√©r√©e avec HfApiModel.")
    return response.content


@st.cache_resource(show_spinner=False)
def load_models():
    """
    Charge et met en cache les mod√®les et l'index FAISS.
    """
    load_dotenv()
    hf_token = os.environ.get("HUGGINGFACE_TOKEN")
    if not hf_token:
        st.error("La variable d'environnement HUGGINGFACE_TOKEN n'est pas d√©finie.")
        raise EnvironmentError("HUGGINGFACE_TOKEN n'est pas d√©fini.")
    login(token=hf_token)
    logging.info("Connect√© √† Hugging Face avec succ√®s.")

    # Chargement du mod√®le d'embeddings
    EMBEDDING_MODEL_NAME = "thenlper/gte-small"
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        multi_process=True,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    logging.info(f"Mod√®le d'embeddings '{EMBEDDING_MODEL_NAME}' charg√©.")

    # Chargement de l'index FAISS
    save_path = "faiss_index_pdf"
    faiss_db = FAISS.load_local(
        save_path,
        embedding_model,
        distance_strategy=DistanceStrategy.COSINE,
        allow_dangerous_deserialization=True
    )
    logging.info("Index FAISS charg√© avec succ√®s.")

    # Choix du backend du mod√®le de langage
    backend = "HfApi"  # Modifier en "transformers" pour utiliser des mod√®les transformers directement.
    if backend == "transformers":
        MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME, torch_dtype=torch.float16, device_map="auto"
        )
        logging.info(f"Mod√®le transformers '{MODEL_NAME}' charg√©.")
    else:
        model_id = "Qwen/Qwen2.5-Coder-32B-Instruct"
        tokenizer = None
        hf_api_token = os.environ.get("HUGGINGFACE_TOKEN")
        if not hf_api_token:
            st.error("La variable d'environnement HUGGINGFACE_TOKEN n'est pas d√©finie.")
            raise EnvironmentError("HUGGINGFACE_TOKEN n'est pas d√©fini.")
        model = HfApiModel(model_id=model_id, token=hf_api_token)
        logging.info(f"Mod√®le HfApi '{model_id}' charg√©.")

    return faiss_db, model, tokenizer


def main():
    st.set_page_config(page_title="EpiRAG", layout="wide")

    # Affichage du logo
    logo_path = "imgs/logo.png"
    if os.path.exists(logo_path):
        st.image(logo_path, width=150)
    else:
        logging.warning("Le fichier de logo n'a pas √©t√© trouv√©.")

    st.title("EpiRAG")
    st.markdown(
        """
        Cette interface vous permet d'engager une conversation avec un assistant qui utilise un m√©canisme de r√©cup√©ration 
        de documents pour fournir des r√©ponses pr√©cises et concises.
        """
    )

    # Chargement des mod√®les et de l'index
    with st.spinner("Chargement des mod√®les et de l'index..."):
        try:
            faiss_db, model, tokenizer = load_models()
        except Exception as e:
            st.error(f"Erreur lors du chargement des mod√®les : {e}")
            logging.exception("Erreur lors du chargement des mod√®les.")
            return

    # Initialisation de l'historique de conversation dans la session
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Affichage de l'historique de conversation
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.chat_message("user").write(msg["content"])
        else:
            st.chat_message("assistant").write(msg["content"])

    # Zone de saisie pour l'utilisateur
    user_input = st.chat_input("Tapez votre message ici")
    if user_input:
        # Enregistrement et affichage du message utilisateur
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.write(user_input)

        # Recherche des documents et g√©n√©ration de la r√©ponse
        with st.spinner("Recherche et g√©n√©ration en cours..."):
            k = 5
            retrieved_docs = faiss_db.similarity_search(query=user_input, k=k)
            logging.info(f"{len(retrieved_docs)} documents r√©cup√©r√©s pour la requ√™te.")
            response = do_generate(model, tokenizer, retrieved_docs, user_input)

        # Affichage de la r√©ponse g√©n√©r√©e
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.write(response)

        # Optionnel : affichage des documents r√©cup√©r√©s dans un expander
        with st.expander("Documents r√©cup√©r√©s"):
            for i, doc in enumerate(retrieved_docs):
                source_file = doc.metadata.get("source", None)
                if source_file:
                    source_link = os.path.join("docs", os.path.basename(source_file))
                    st.markdown(f"[üìÑ Acc√©der au document {i+1}]({source_link})", unsafe_allow_html=True)
                st.write(doc.page_content)


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:
        logging.exception("Une erreur est survenue lors de l'ex√©cution de l'application.")
        st.error(f"Une erreur est survenue : {exc}")
